{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Whitening Layer\n",
    "The purpose of this notebook is to implement the batch whitening layer.   \n",
    "The implementation is inspired by the implementation of BatchNorm layer from [this reference](https://d2l.ai/chapter_convolutional-modern/batch-norm.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device to use: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(\"Device to use:\", device)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization  \n",
    "Lets start with implementing BatchNorm from scratch and test it on a simple dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, running_mean, running_var, eps, momentum):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    assert len(X.shape) in (2, 4)\n",
    "    if len(X.shape) == 2:\n",
    "        shape = (1, X.shape[1])\n",
    "    else:\n",
    "        shape = (1, X.shape[1], 1, 1)\n",
    "\n",
    "    if not torch.is_grad_enabled():\n",
    "        # In prediction mode, use mean and variance obtained by moving average\n",
    "        X_hat = (X - running_mean) / torch.sqrt(running_var + eps)\n",
    "    else:\n",
    "        if len(X.shape) == 2:\n",
    "            # When using a fully connected layer, calculate the mean and\n",
    "            # variance on the feature dimension\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # When using a two-dimensional convolutional layer, calculate the\n",
    "            # mean and variance on the channel dimension (axis=1). Here we\n",
    "            # need to maintain the shape of X, so that the broadcasting\n",
    "            # operation can be carried out later\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        # In training mode, the current mean and variance are used\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # Update the mean and variance using moving average\n",
    "        running_mean = (1.0 - momentum) * running_mean + momentum * mean\n",
    "        running_var = (1.0 - momentum) * running_var + momentum * var\n",
    "    Y = gamma.view(shape) * X_hat + beta.view(shape)  # Scale and shift\n",
    "    return Y, running_mean.data, running_var.data\n",
    "\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        # if num_dims == 2:\n",
    "        #     shape = (1, num_features)\n",
    "        # else:\n",
    "        #     shape = (1, num_features, 1, 1)\n",
    "        shape = num_features\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "\n",
    "        # The variables that are not model parameters are initialized to 0 and 1\n",
    "        # self.running_mean = torch.zeros(shape)\n",
    "        # self.running_var = torch.ones(shape)\n",
    "        self.register_buffer('running_mean', torch.zeros(shape))\n",
    "        self.register_buffer('running_var', torch.ones(shape))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        if self.running_mean.device != X.device:\n",
    "            self.running_mean = self.running_mean.to(X.device)\n",
    "            self.running_var = self.running_var.to(X.device)\n",
    "        # Save the updated running_mean and moving_var\n",
    "        Y, self.running_mean, self.running_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.running_mean,\n",
    "            self.running_var, eps=1e-5, momentum=0.1)\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super(BatchNorm2d, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        # Learnable parameters\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "        # Running mean and variance\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            # Calculate mean and variance for the batch\n",
    "            mean = x.mean([0, 2, 3], keepdim=True)\n",
    "            var = x.var([0, 2, 3], keepdim=True, unbiased=False)\n",
    "            # Update running mean and variance\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        \n",
    "        # Normalize\n",
    "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        # Scale and shift\n",
    "        out = self.gamma.view(1, self.num_features, 1, 1) * x_normalized + self.beta.view(1, self.num_features, 1, 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of 2D images (batch size, channels, height, width)\n",
    "x = torch.randn(20, 10, 50, 50)\n",
    "\n",
    "# Our custom batch normalization layer\n",
    "custom_bn = BatchNorm(num_features=10, num_dims=4)\n",
    "# custom_bn = BatchNorm2d(num_features=10)\n",
    "\n",
    "# PyTorch's built-in batch normalization layer\n",
    "torch_bn = nn.BatchNorm2d(num_features=10)\n",
    "\n",
    "\n",
    "# Copy the parameters from our custom layer to the built-in layer for a fair comparison\n",
    "torch_bn.weight.data = custom_bn.gamma.data.clone()\n",
    "torch_bn.bias.data = custom_bn.beta.data.clone()\n",
    "torch_bn.running_mean = custom_bn.running_mean.clone()\n",
    "torch_bn.running_var = custom_bn.running_var.clone()\n",
    "\n",
    "# Forward pass\n",
    "custom_bn_output = custom_bn(x)\n",
    "torch_bn_output = torch_bn(x)\n",
    "\n",
    "# Check if the outputs are close\n",
    "assert torch.allclose(custom_bn_output, torch_bn_output, atol=1e-5), \"The outputs are not close enough!\"\n",
    "\n",
    "print(\"Functional validation passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.var([0, 2, 3], keepdim=True, unbiased=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bn.training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Whitening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_to_corr(cov_matrix):\n",
    "    # Compute the standard deviations\n",
    "    std = torch.sqrt(torch.diag(cov_matrix))\n",
    "    \n",
    "    # Compute the correlation matrix\n",
    "    corr_matrix = cov_matrix / torch.outer(std, std)\n",
    "    \n",
    "    # Extract upper triangular part (excluding diagonal)\n",
    "    upper_tri = torch.triu(corr_matrix, diagonal=1)\n",
    "    \n",
    "    # Compute average of cross-correlation coefficients\n",
    "    avg_corr = upper_tri.abs().sum() / ((upper_tri.numel() - upper_tri.diag().numel())/2)\n",
    "    \n",
    "    return corr_matrix, avg_corr\n",
    "\n",
    "\n",
    "def corr_to_cov(corr_matrix,std):\n",
    "    # Compute the standard deviations\n",
    "    D = torch.diag(std)\n",
    "    # D=torch.outer(std,std)\n",
    "    # cov=corr_matrix * torch.outer(std,std)\n",
    "    return D@corr_matrix@D\n",
    "\n",
    "\n",
    "def fix_corr(corr):\n",
    "    a=0.9+0.1*torch.exp(-(abs(corr)/0.9)**10)\n",
    "    a=a.clone()  # so not to lose the gradients in backprop\n",
    "    torch.diagonal(a).fill_(1.0)\n",
    "    return a*corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choleski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### diagonal Cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_orthonorm_obsolete(X, gamma, beta, running_mean=None, running_cov=None, eps=1e-5, momentum=0.1):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    assert len(X.shape) in (2, 4)\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    if len(X.shape) == 2:\n",
    "        # When using a fully connected layer, calculate the mean and\n",
    "        # variance on the feature dimension\n",
    "        shape = (1, n_features)\n",
    "        mean = X.mean(dim=0)\n",
    "        cov = torch.cov(X.T,correction=0)        \n",
    "        # var = ((X - mean) ** 2).mean(dim=0)\n",
    "    else:\n",
    "        # When using a two-dimensional convolutional layer, calculate the\n",
    "        # mean and covariance on the channel dimension (axis=1). Here we\n",
    "        # need to maintain the shape of X, so that the broadcasting\n",
    "        # operation can be carried out later\n",
    "        shape = (1, n_features, 1, 1)\n",
    "        mean = X.mean(dim=(0, 2, 3))\n",
    "        Xtmp = X.view(X.shape[0],X.shape[1],-1)\n",
    "        Xtmp = Xtmp.permute(1,0,2).reshape(X.shape[1],-1)\n",
    "        cov = torch.cov(Xtmp,correction=0) \n",
    "        # var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "    # In training mode, the current mean and variance are used\n",
    "    # Update the mean and variance using moving average\n",
    "    running_mean = (1.0 - momentum) * running_mean + momentum * mean\n",
    "    running_cov = (1.0 - momentum) * running_cov + momentum * cov\n",
    "    L = torch.linalg.cholesky(running_cov + eps*torch.eye(n_features))\n",
    "    if len(X.shape) == 2:\n",
    "        X_hat = (X-running_mean.view(1,n_features)).T\n",
    "        Y = torch.linalg.solve_triangular(L,X_hat,upper=False).T\n",
    "    else:\n",
    "        X_hat = X-running_mean.view(1,n_features,1,1)\n",
    "        X_hat = X_hat.permute(1,0,2,3).reshape(X.shape[1],-1)\n",
    "        Y = torch.linalg.solve_triangular(L,X_hat,upper=False).reshape(X.shape[1],X.shape[0],X.shape[2],X.shape[3]).permute(1,0,2,3)\n",
    "    # Y = gamma.view(shape) * Y + beta.view(shape)  # Scale and shift\n",
    "    return Y, running_mean.data, running_cov.data\n",
    "\n",
    "def batch_orthonorm(X, gamma, beta, running_mean=None, running_cov=None, eps=1e-5, momentum=0.1):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    assert len(X.shape) in (2, 4)\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    if len(X.shape) == 2:\n",
    "        # When using a fully connected layer, calculate the mean and\n",
    "        # variance on the feature dimension\n",
    "        shape = (1, n_features)\n",
    "        mean = X.mean(dim=0)\n",
    "        cov = torch.cov(X.T,correction=0)        \n",
    "        # var = ((X - mean) ** 2).mean(dim=0)\n",
    "    else:\n",
    "        # When using a two-dimensional convolutional layer, calculate the\n",
    "        # mean and covariance on the channel dimension (axis=1). Here we\n",
    "        # need to maintain the shape of X, so that the broadcasting\n",
    "        # operation can be carried out later\n",
    "        shape = (1, n_features, 1, 1)\n",
    "        mean = X.mean(dim=(0, 2, 3))\n",
    "        Xtmp = X.view(X.shape[0],X.shape[1],-1)\n",
    "        Xtmp = Xtmp.permute(1,0,2).reshape(X.shape[1],-1)\n",
    "        cov = torch.cov(Xtmp,correction=0) \n",
    "        # var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "    # In training mode, the current mean and variance are used\n",
    "    # Update the mean and variance using moving average\n",
    "    if torch.is_grad_enabled():\n",
    "        running_mean = (1.0 - momentum) * running_mean + momentum * mean\n",
    "        running_cov = (1.0 - momentum) * running_cov + momentum * cov\n",
    "        L = torch.linalg.cholesky(cov + eps*torch.eye(n_features))\n",
    "        if len(X.shape) == 2:\n",
    "            X_hat = (X-mean.view(1,n_features)).T\n",
    "            Y = torch.linalg.solve_triangular(L,X_hat,upper=False).T\n",
    "        else:\n",
    "            X_hat = X-mean.view(1,n_features,1,1)\n",
    "            X_hat = X_hat.permute(1,0,2,3).reshape(X.shape[1],-1)\n",
    "            Y = torch.linalg.solve_triangular(L,X_hat,upper=False).reshape(X.shape[1],X.shape[0],X.shape[2],X.shape[3]).permute(1,0,2,3)\n",
    "    else:\n",
    "        L = torch.linalg.cholesky(running_cov + eps*torch.eye(n_features))\n",
    "        if len(X.shape) == 2:\n",
    "            X_hat = (X-running_mean.view(1,n_features)).T\n",
    "            Y = torch.linalg.solve_triangular(L,X_hat,upper=False).T\n",
    "        else:\n",
    "            X_hat = X-running_mean.view(1,n_features,1,1)\n",
    "            X_hat = X_hat.permute(1,0,2,3).reshape(X.shape[1],-1)\n",
    "            Y = torch.linalg.solve_triangular(L,X_hat,upper=False).reshape(X.shape[1],X.shape[0],X.shape[2],X.shape[3]).permute(1,0,2,3)\n",
    "    # Y = gamma.view(shape) * Y + beta.view(shape)  # Scale and shift\n",
    "    return Y, running_mean.data, running_cov.data\n",
    "\n",
    "class BatchWhitening(nn.Module):\n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features,momentum=0.1):\n",
    "        super().__init__()\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.momentum = momentum\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "        # The variables that are not model parameters are initialized to 0 and 1\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_cov', torch.eye(num_features))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        if self.running_mean.device != X.device:\n",
    "            self.running_mean = self.running_mean.to(X.device)\n",
    "            self.running_cov = self.running_cov.to(X.device)\n",
    "        # Save the updated running_mean and moving_var\n",
    "        # Y, self.running_mean, self.running_var = batch_orthonorm(\n",
    "        #     X, self.gamma, self.beta, self.running_mean,\n",
    "        #     self.running_cov, eps=1e-5, momentum=0.1)\n",
    "        Y, self.running_mean, self.running_cov = batch_orthonorm(\n",
    "            X, self.gamma, self.beta, self.running_mean,\n",
    "            self.running_cov, eps=1e-5, momentum=self.momentum)\n",
    "\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Block-diagonal Cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grp_ch(num_features,num_groups=1,num_channels=-1):\n",
    "    if num_channels == -1:\n",
    "        num_channels = (num_features - 1) // num_groups + 1\n",
    "    num_groups = num_features // num_channels\n",
    "    while num_features % num_channels != 0:\n",
    "        num_channels //= 2\n",
    "        num_groups = num_features // num_channels\n",
    "    assert num_groups > 0 and num_features % num_groups == 0, \"num features={}, num groups={}\".format(num_features,\n",
    "        num_groups)\n",
    "    return num_groups,num_channels\n",
    "\n",
    "def block_cholesky_batch(X, running_mean=None, running_cov=None, n_channels=-1, eps=1e-5, momentum=0.1,cov_warmup=False):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    assert len(X.shape) in (2, 4)\n",
    "    n_features = X.shape[1]\n",
    "    if n_channels==-1:\n",
    "        n_channels=n_features\n",
    "    n_groups=n_features//n_channels\n",
    "    x = X.transpose(0, 1).contiguous().view(n_groups, n_channels, -1)    \n",
    "    cov_I = torch.eye(n_channels).expand(n_groups, n_channels, n_channels).to(running_cov.device)     \n",
    "    _, d, m = x.size()\n",
    "    mean = x.mean(-1, keepdim=True)\n",
    "    xc = x - mean\n",
    "    cov = torch.baddbmm(cov_I, xc, xc.transpose(1, 2), beta=eps, alpha=1. / m)\n",
    "\n",
    "    # In training mode, the current mean and variance are used\n",
    "    # Update the mean and variance using moving average\n",
    "    # running_mean = (1.0 - momentum) * running_mean + momentum * mean\n",
    "    running_mean = mean         # no running mean (alpha = momentum = 1)\n",
    "    \n",
    "    # during warmup, we're not updating running_cov but using a statistics of current batch \n",
    "    if cov_warmup:\n",
    "        # x_var = torch.diag_embed(torch.diag(cov))\n",
    "        x_var = torch.eye(d).unsqueeze(0)*cov\n",
    "        running_cov = (1.0 - momentum) * x_var + momentum * cov\n",
    "    # when warm up is done, running_cov is updated only during training\n",
    "    elif torch.is_grad_enabled():    \n",
    "        running_cov = (1.0 - momentum) * running_cov + momentum * cov\n",
    "    # note that we're using running_cov also during training\n",
    "    # L = torch.linalg.cholesky(running_cov + eps*cov_I)\n",
    "    L = torch.linalg.cholesky(running_cov)\n",
    "    if len(X.shape) == 2:\n",
    "        Y = torch.linalg.solve_triangular(L,xc,upper=False).reshape(X.shape[1],X.shape[0]).permute(1,0)\n",
    "    else:\n",
    "        Y = torch.linalg.solve_triangular(L,xc,upper=False).reshape(X.shape[1],X.shape[0],X.shape[2],X.shape[3]).permute(1,0,2,3)\n",
    "    return Y, running_mean.data, running_cov.data\n",
    "\n",
    "\n",
    "class BatchWhitening(nn.Module):\n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features,num_groups=1, num_channels=-1, momentum=0.1):\n",
    "        super().__init__()\n",
    "        self.num_groups,self.num_channels=get_grp_ch(num_features,num_groups,num_channels)\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.momentum = momentum\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "        # The variables that are not model parameters are initialized to 0 and 1\n",
    "        self.register_buffer('running_mean', torch.zeros(self.num_groups, self.num_channels, 1))\n",
    "        self.register_buffer('running_cov', torch.eye(self.num_channels).expand(self.num_groups, self.num_channels, self.num_channels))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        if self.running_mean.device != X.device:\n",
    "            self.running_mean = self.running_mean.to(X.device)\n",
    "            self.running_cov = self.running_cov.to(X.device)\n",
    "        # Save the updated running_mean and moving_var\n",
    "        Y, self.running_mean, self.running_cov = block_cholesky_batch(\n",
    "            X, self.running_mean, self.running_cov, self.num_channels, eps=1e-5, momentum=self.momentum)\n",
    "\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_groups=4\n",
    "num_channels=3\n",
    "xx=torch.eye(num_channels).expand(num_groups, num_channels, num_channels)\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IterNorm\n",
    "\n",
    "The algorithm :  (taken from [the paper](https://arxiv.org/abs/1904.03441) )  \n",
    "![iternorm](./iternorm_algo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "\n",
    "def fix_corr(corr):\n",
    "    a=0.9+0.1*torch.exp(-(abs(corr)/0.9)**10)\n",
    "    a=a.clone()  # so not to lose the gradients in backprop\n",
    "    torch.diagonal(a).fill_(1.0)\n",
    "    return a*corr\n",
    "\n",
    "def get_grp_ch(num_features,num_groups=1,num_channels=-1):\n",
    "    if num_channels == -1:\n",
    "        num_channels = (num_features - 1) // num_groups + 1\n",
    "    num_groups = num_features // num_channels\n",
    "    while num_features % num_channels != 0:\n",
    "        num_channels //= 2\n",
    "        num_groups = num_features // num_channels\n",
    "    assert num_groups > 0 and num_features % num_groups == 0, \"num features={}, num groups={}\".format(num_features,\n",
    "        num_groups)\n",
    "    return num_groups,num_channels\n",
    "\n",
    "\n",
    "class IterNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_groups=1, num_channels=-1, T=5, dim=4, eps=1e-5, momentum=0.1, affine=True, *args, **kwargs):\n",
    "        super(IterNorm, self).__init__()\n",
    "        # assert dim == 4, 'IterNorm is not support 2D'\n",
    "        self.T = T\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.num_features = num_features\n",
    "        self.affine = affine\n",
    "        self.dim = dim\n",
    "        self.fix_cov=kwargs.get('fix_cov',False)\n",
    "\n",
    "        num_groups,num_channels=get_grp_ch(num_features,num_groups,num_channels)\n",
    "        self.num_groups = num_groups\n",
    "        self.num_channels = num_channels\n",
    "        shape = [1] * dim\n",
    "        shape[1] = self.num_features\n",
    "        if self.affine:\n",
    "            self.weight = Parameter(torch.Tensor(*shape))\n",
    "            self.bias = Parameter(torch.Tensor(*shape))\n",
    "\n",
    "        if not self.affine:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.register_buffer('running_mean', torch.zeros(num_groups, num_channels, 1))\n",
    "        # running whiten matrix\n",
    "        self.register_buffer('running_wm', torch.eye(num_channels).expand(num_groups, num_channels, num_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # self.reset_running_stats()\n",
    "        if self.affine:\n",
    "            self.weight.data.fill_(1.0)\n",
    "            self.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        eps = 1e-5\n",
    "        momentum = self.momentum\n",
    "\n",
    "        nc = self.num_channels\n",
    "        T = self.T\n",
    "        g = X.size(1) // nc\n",
    "        x = X.transpose(0, 1).contiguous().view(g, nc, -1)\n",
    "        _, d, m = x.size()\n",
    "        if self.training:\n",
    "            # calculate centered activation by subtracted mini-batch mean\n",
    "            mean = x.mean(-1, keepdim=True)\n",
    "            xc = x - mean\n",
    "            # calculate covariance matrix\n",
    "            P = [None] * (T + 1)\n",
    "            P[0] = torch.eye(d).to(X).expand(g, d, d)\n",
    "            # Sigma = torch.baddbmm(eps, P[0], 1. / m, xc, xc.transpose(1, 2))\n",
    "            Sigma = torch.baddbmm(P[0], xc, xc.transpose(1, 2), beta=eps, alpha=1. / m)  # =torch.cov(xc,correction=0)\n",
    "            if self.fix_cov:\n",
    "                Sigma[0]=fix_corr(Sigma[0])\n",
    "            # reciprocal of trace of Sigma: shape [g, 1, 1]\n",
    "            rTr = (Sigma * P[0]).sum(1, keepdim=True).sum(2, keepdim=True).reciprocal_()\n",
    "            Sigma_N = Sigma * rTr\n",
    "            for k in range(T):\n",
    "                # P[k + 1] = torch.baddbmm(1.5, P[k], -0.5, P[k].bmm(P[k]).bmm(P[k]), Sigma_N)\n",
    "                P[k + 1] = torch.baddbmm(P[k], P[k].bmm(P[k]).bmm(P[k]), Sigma_N, beta=1.5, alpha=-0.5)\n",
    "            wm = P[T].mul_(rTr.sqrt())  # whiten matrix: the matrix inverse of Sigma, i.e., Sigma^{-1/2}\n",
    "            # self.running_mean += momentum * ( mean.detach() - self.running_mean)\n",
    "            # self.running_wm += momentum * ( wm.detach() - self.running_wm)\n",
    "            self.running_mean = (1-momentum)*self.running_mean + momentum * mean.detach()\n",
    "            self.running_wm = (1-momentum)*self.running_wm + momentum * wm.detach() \n",
    "        else:\n",
    "            xc = x - self.running_mean\n",
    "            wm = self.running_wm\n",
    "        xn = wm.matmul(xc)\n",
    "        X_hat = xn.view(X.size(1), X.size(0), *X.size()[2:]).transpose(0, 1).contiguous()\n",
    "        \n",
    "        # affine\n",
    "        if self.affine:\n",
    "            X_hat = X_hat * self.weight\n",
    "            X_hat = X_hat + self.bias\n",
    "        return X_hat\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return '{num_features}, num_channels={num_channels}, T={T}, eps={eps}, dim={dim}, ' \\\n",
    "               'momentum={momentum}, affine={affine}'.format(**self.__dict__)\n",
    "\n",
    "from batch_whitening import IterNormMod,iter_norm_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grp,n_ch=get_ch_grp(65,3)\n",
    "print(f'{n_grp} groups, {n_ch} channels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [wikipedia](https://en.wikipedia.org/wiki/Covariance_matrix):  \n",
    "The covariance matrix is given by:  \n",
    "![cov](cov.png)\n",
    "\n",
    "and the correlation matrix is given by:  \n",
    "![corr](corr_mat.png)\n",
    "\n",
    "so if we have a matrix of `N` samples of a random vector $x \\in \\mathbb{R}^C$ , we can compute the covariance of the random vector as follows:\n",
    "`corr_matrix = torch.corrcoef(x_f)`  \n",
    "and we can also compute the covariance matrix with: `cov_matrix = x.cov()`   \n",
    "\n",
    "we can also get `corr_matrix` from `cov_matrix` using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_well_conditioned_covariance(n, condition_number=2):\n",
    "    # Step 1: Generate a random matrix\n",
    "    # L = np.random.rand(n, n)\n",
    "    L = torch.rand(n, n)\n",
    "    \n",
    "    # Step 2: Make it lower triangular with positive diagonal\n",
    "    L = torch.tril(L) + n * torch.eye(n)\n",
    "    \n",
    "    # Step 3: Construct the symmetric positive definite matrix\n",
    "    A = L@L.T\n",
    "    \n",
    "    # Step 4: Adjust the condition number\n",
    "    # Scale the matrix to have a desired condition number\n",
    "    u, s, vh = torch.linalg.svd(A)\n",
    "    s = torch.linspace(s[0], s[0] / condition_number, len(s))\n",
    "    A = (u * s)@vh\n",
    "    \n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11.9165,  7.4184,  2.5906],\n",
      "        [ 7.4184,  7.5860,  0.2976],\n",
      "        [ 2.5906,  0.2976, 10.4384]])\n",
      "condition number: 10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([18.1460+0.j,  1.8146+0.j,  9.9803+0.j])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov=generate_well_conditioned_covariance(3,10)\n",
    "print(cov)\n",
    "print(f\"condition number: {torch.linalg.cond(cov)}\")\n",
    "torch.linalg.eig(cov)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following compute the rank of the matrix and the average of the correlation coefficients in the correlation matrix:\n",
    "# x is expected to have the following shape: [B,C,H,W] if it has 4 dimensions, [B,C] if it has 2 dimensions \n",
    "def rank_and_avg_corr(x):\n",
    "    if len(x.shape)==4:  # conv\n",
    "        # flatten x from [B,C,H,W] to [C,B*H*W]\n",
    "        x_f= x.permute(1,0,2,3).reshape(x.shape[1],-1)\n",
    "    else:\n",
    "        # change from [B,C] to [C,B]\n",
    "        x_f=x.T\n",
    "    # compute corr matrix\n",
    "    corr_matrix = torch.corrcoef(x_f)\n",
    "    # Extract upper triangular part (excluding diagonal)\n",
    "    upper_tri = torch.triu(corr_matrix, diagonal=1)\n",
    "    # Compute average of cross-correlation coefficients\n",
    "    avg_corr = upper_tri.sum() / (upper_tri.numel() - upper_tri.diag().numel())\n",
    "    rank = torch.linalg.matrix_rank(x_f)/x.shape[1]\n",
    "    return rank,avg_corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating a tensor of shape (100, 3,32,32) with mean tensor([[22., 70., 90.]]) and covariance \n",
      " tensor([[ 2.4187,  5.7671,  2.1641],\n",
      "        [ 5.7671, 15.1461,  1.8976],\n",
      "        [ 2.1641,  1.8975,  9.6556]]):\n",
      "actual mean and cov: tensor([22.0045, 70.0106, 90.0045]),\n",
      " tensor([[ 2.4134,  5.7619,  2.1426],\n",
      "        [ 5.7619, 15.1526,  1.8483],\n",
      "        [ 2.1426,  1.8483,  9.6343]])\n"
     ]
    }
   ],
   "source": [
    "# Create a batch of 2D images (batch size, channels, height, width)\n",
    "num_features = 3\n",
    "n_samples=100\n",
    "m = torch.randint(10,100,(1,num_features)).float()\n",
    "c = torch.randint(1,10,(1,num_features))\n",
    "# cov = c.T@c + 0.1*torch.eye(num_features)       # ill conditioned \n",
    "cov = generate_well_conditioned_covariance(num_features,10000)  # well conditioned\n",
    "\n",
    "print(f'generating a tensor of shape ({n_samples}, {num_features},32,32) with mean {m} and covariance \\n {cov}:')\n",
    "x = torch.randn(n_samples, num_features, 32, 32).permute(1,0,2,3).reshape(num_features,-1).T\n",
    "L = torch.linalg.cholesky(cov.float())\n",
    "xc= x@L.T + m\n",
    "# xc[:,-1]=0.5*xc[:,-2]+0.5*xc[:,-3]\n",
    "# print(f'actual mean and cov: {xc.mean(0)},\\n {torch.cov(xc.T,correction=0)}')\n",
    "print(f'actual mean and cov: {xc.mean(0)},\\n {torch.cov(xc.T)}')\n",
    "x_c = xc.permute(1,0).reshape(num_features,n_samples,32,32).permute(1,0,2,3)\n",
    "# flatten x_c to [num_featurs,num_samples]:\n",
    "x_c_f= x_c.permute(1,0,2,3).reshape(x_c.shape[1],-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.4134,  5.7619,  2.1426],\n",
      "        [ 5.7619, 15.1526,  1.8483],\n",
      "        [ 2.1426,  1.8483,  9.6343]])\n",
      " the eigen values of the cov matrix are : tensor([1.8224e-03, 9.0838e+00, 1.8115e+01])\n",
      "condition number: 9935.677734375\n",
      "\n",
      " \n",
      " correlation matrix and average cross correlation:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 0.9528, 0.4443],\n",
       "         [0.9528, 1.0000, 0.1530],\n",
       "         [0.4443, 0.1530, 1.0000]]),\n",
       " tensor(0.5167))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the cov from x_c:\n",
    "x_c_cov = x_c_f.cov()\n",
    "print(x_c_cov)\n",
    "print(f' the eigen values of the cov matrix are : {torch.linalg.eigh(x_c_cov)[0]}')\n",
    "print(f\"condition number: {torch.linalg.cond(x_c_cov)}\")\n",
    "print(\"\\n \\n correlation matrix and average cross correlation:\") \n",
    "cov_to_corr(x_c_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the correlation coefficient matrix\n",
    "print(torch.corrcoef(x_c_f))\n",
    "# test the conversion function: should be the same output\n",
    "x_c_corr=cov_to_corr(x_c_cov)[0]\n",
    "x_c_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_to_cov(x_c_corr,x_c_f.std(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xc.shape)\n",
    "print(x_c.shape)\n",
    "print(x_c_f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validating Batch Whitening layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_w_cov matrix: \n",
      " tensor([[ 1.0000e+00,  1.1508e-05,  1.2478e-04],\n",
      "        [ 1.1508e-05,  9.9997e-01, -4.2383e-04],\n",
      "        [ 1.2478e-04, -4.2383e-04,  9.9543e-01]])\n",
      "eigen values of x_w_cov matrix: \n",
      " tensor([0.9954, 1.0000, 1.0000])\n",
      "condition number: 1.0046452283859253\n",
      "correlation matrix and average cross correlation: \n",
      " tensor([[ 1.0000e+00,  1.1508e-05,  1.2507e-04],\n",
      "        [ 1.1508e-05,  1.0000e+00, -4.2481e-04],\n",
      "        [ 1.2507e-04, -4.2481e-04,  1.0000e+00]]) \n",
      " 0.00018712806922849268\n",
      "Functional validation passed!\n"
     ]
    }
   ],
   "source": [
    "# Our custom batch normalization layer\n",
    "bw_layer = BatchWhitening(num_features,momentum=1)\n",
    "\n",
    "# Forward pass\n",
    "x_w = bw_layer(x_c)   # expecting x_c.shape=[N,C,H,W] \n",
    "\n",
    "x_w_cov = x_w.permute(1,0,2,3).reshape(x.shape[1],-1).cov()\n",
    "print(f\"x_w_cov matrix: \\n {x_w_cov}\")\n",
    "print(f\"eigen values of x_w_cov matrix: \\n {torch.linalg.eigh(x_w_cov)[0]}\")\n",
    "print(f\"condition number: {torch.linalg.cond(x_w_cov)}\")\n",
    "x_w_corr,x_w_avgxcorr = cov_to_corr(x_w_cov)\n",
    "print(f\"correlation matrix and average cross correlation: \\n {x_w_corr} \\n {x_w_avgxcorr}\")\n",
    "# Check if the outputs are indeed orthonormal\n",
    "assert torch.allclose(x_w_cov, torch.eye(num_features), atol=1e-2), \"The outputs are not close enough!\"\n",
    "print(\"Functional validation passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the layer also normalized the input, the correlation coefficient matrix should be nearly the same (up to approx errors) as the covariance matrix. lets check it:\n",
    "print(cov_to_corr(x_w_cov))\n",
    "# alternatice way to compute: - should produce the same output\n",
    "print(torch.corrcoef(x_w.permute(1,0,2,3).reshape(x.shape[1],-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_c_cov = x_c.permute(1,0,2,3).reshape(x.shape[1],-1).cov()\n",
    "# we can also compute x_c_cov as follows\n",
    "# torch.cov(x_c.permute(1,0,2,3).reshape(x.shape[1],-1))\n",
    "print(x_c_cov)\n",
    "cov_to_corr(x_c_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the rank\n",
    "torch.linalg.matrix_rank(x_c.permute(1,0,2,3).reshape(x.shape[1],-1))/x_c.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validating iter norm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ItN = IterNormMod(3, num_groups=1, T=11, momentum=1, affine=False,fix_cov=True)\n",
    "print(ItN)\n",
    "ItN.train()\n",
    "x_c.requires_grad_()\n",
    "x_w = ItN(x_c)\n",
    "print(x_w.shape)\n",
    "x_w_cov = x_w.permute(1,0,2,3).reshape(x_w.shape[1],-1).cov()\n",
    "\n",
    "print(f\"x_w_cov matrix: \\n {x_w_cov}\")\n",
    "print(f\"eigen values of x_w_cov matrix: \\n {torch.linalg.eigh(x_w_cov)[0]}\")\n",
    "print(f\"condition number: {torch.linalg.cond(x_w_cov)}\")\n",
    "x_w_corr,x_w_avgxcorr = cov_to_corr(x_w_cov)\n",
    "print(f\"correlation matrix and average cross correlation: \\n {x_w_corr} \\n {x_w_avgxcorr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.cond(x_w_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_w_cov.diag())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.ones_like(x_w_cov)*0.9\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.fill_diagonal_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.corrcoef(x_w.permute(1,0,2,3).reshape(x_w.shape[1],-1))\n",
    "cov_to_corr(x_w_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating a tensor of shape (B, 64) with mean tensor([[33., 39., 10., 79., 57., 32., 30., 40., 98., 78., 29., 87., 45., 21.,\n",
      "         94., 36., 95., 13., 90., 45., 68., 57., 79., 61., 35., 77., 21., 28.,\n",
      "         50., 51., 66., 33., 67., 35., 76., 71., 43., 46., 80., 51., 53., 30.,\n",
      "         92., 38., 12., 43., 73., 52., 94., 30., 31., 78., 81., 79., 33., 22.,\n",
      "         52., 79., 56., 99., 27., 98., 81., 86.]]) and covariance \n",
      " tensor([[ 3.1777e+03, -1.4015e+02, -2.0016e+02,  ...,  3.8941e+02,\n",
      "          2.6877e+02,  8.4795e+01],\n",
      "        [-1.4015e+02,  3.2509e+03, -2.0779e+01,  ...,  2.3790e+02,\n",
      "         -1.1304e+02,  3.5499e+02],\n",
      "        [-2.0016e+02, -2.0778e+01,  3.1022e+03,  ..., -7.7150e+01,\n",
      "         -1.9786e+02,  3.3294e+02],\n",
      "        ...,\n",
      "        [ 3.8941e+02,  2.3790e+02, -7.7151e+01,  ...,  3.0220e+03,\n",
      "         -1.9773e+02, -2.7923e+00],\n",
      "        [ 2.6878e+02, -1.1304e+02, -1.9786e+02,  ..., -1.9773e+02,\n",
      "          4.0355e+03, -2.1156e+02],\n",
      "        [ 8.4796e+01,  3.5499e+02,  3.3294e+02,  ..., -2.7921e+00,\n",
      "         -2.1155e+02,  3.5623e+03]]):\n",
      "tensor xc.shape=torch.Size([1000, 64])\n",
      "actual mean and cov: tensor([ 33.5475,  37.0992,   7.4748,  79.4966,  56.0199,  31.9746,  30.8526,\n",
      "         39.7333,  97.3599,  74.6271,  28.0845,  87.6792,  44.3095,  20.3944,\n",
      "         93.6751,  38.8980,  95.7828,  13.2454,  91.6539,  45.4190,  67.7574,\n",
      "         58.8744,  79.7307,  58.4313,  34.0011,  75.7867,  18.9184,  27.9301,\n",
      "         44.7630,  49.7856,  66.2664,  34.6663,  65.0819,  37.4968,  72.4616,\n",
      "         70.3584,  40.9282,  45.6122,  79.1268,  52.4523,  51.1666,  29.9200,\n",
      "         89.5792,  37.0482,  11.6208,  45.2620,  73.7846,  52.0149,  96.1002,\n",
      "         31.7932,  31.0490,  79.1720,  83.9820,  80.7013,  33.7970,  18.3559,\n",
      "         54.1835,  78.3966,  55.4439,  99.2468,  27.4498, 100.8560,  82.5819,\n",
      "         84.8023]),\n",
      " tensor([[ 3.1477e+03, -3.1403e+02, -2.5581e+02,  ...,  4.4637e+02,\n",
      "         -3.5269e+01,  1.6182e+02],\n",
      "        [-3.1403e+02,  3.1783e+03, -7.7322e+01,  ...,  7.3933e+01,\n",
      "         -1.5754e+00,  3.6731e+02],\n",
      "        [-2.5581e+02, -7.7322e+01,  2.8857e+03,  ..., -2.0473e+02,\n",
      "         -6.7065e+01,  2.9217e+02],\n",
      "        ...,\n",
      "        [ 4.4637e+02,  7.3933e+01, -2.0473e+02,  ...,  3.0763e+03,\n",
      "         -2.1518e+02, -1.3461e+02],\n",
      "        [-3.5269e+01, -1.5754e+00, -6.7065e+01,  ..., -2.1518e+02,\n",
      "          4.0975e+03, -1.3811e+02],\n",
      "        [ 1.6182e+02,  3.6731e+02,  2.9217e+02,  ..., -1.3461e+02,\n",
      "         -1.3811e+02,  3.5700e+03]])\n"
     ]
    }
   ],
   "source": [
    "# Create a batch of vectors (B, num_features)\n",
    "# num_features = 4\n",
    "num_features = 64\n",
    "m = torch.randint(10,100,(1,num_features)).float()\n",
    "c = torch.randint(1,10,(1,num_features))\n",
    "# cov = c.T@c + 0.1*torch.eye(num_features)\n",
    "cov = generate_well_conditioned_covariance(num_features,100)\n",
    "\n",
    "\n",
    "print(f'generating a tensor of shape (B, {num_features}) with mean {m} and covariance \\n {cov}:')\n",
    "x = torch.randn(1000, num_features)\n",
    "# x = torch.randn(128, num_features)\n",
    "L = torch.linalg.cholesky(cov.float())\n",
    "xc= x@L.T + m \n",
    "print(f'tensor xc.shape={xc.shape}')\n",
    "print(f'actual mean and cov: {xc.mean(0)},\\n {torch.cov(xc.T)}')\n",
    "\n",
    "# print(f'rank and average correlation of x: {rank_and_avg_corr(x)}')\n",
    "# print(f'rank and average correlation of xc: {rank_and_avg_corr(xc)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition number: 115.12806701660156\n",
      "\n",
      " \n",
      " correlation matrix and average cross correlation:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.0000e+00, -9.9283e-02, -8.4878e-02,  ...,  1.4344e-01,\n",
       "          -9.8207e-03,  4.8272e-02],\n",
       "         [-9.9283e-02,  1.0000e+00, -2.5531e-02,  ...,  2.3644e-02,\n",
       "          -4.3655e-04,  1.0904e-01],\n",
       "         [-8.4878e-02, -2.5531e-02,  1.0000e+00,  ..., -6.8715e-02,\n",
       "          -1.9503e-02,  9.1029e-02],\n",
       "         ...,\n",
       "         [ 1.4344e-01,  2.3644e-02, -6.8715e-02,  ...,  1.0000e+00,\n",
       "          -6.0609e-02, -4.0620e-02],\n",
       "         [-9.8207e-03, -4.3655e-04, -1.9503e-02,  ..., -6.0609e-02,\n",
       "           1.0000e+00, -3.6111e-02],\n",
       "         [ 4.8272e-02,  1.0904e-01,  9.1029e-02,  ..., -4.0620e-02,\n",
       "          -3.6111e-02,  1.0000e+00]]),\n",
       " tensor(0.0651))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_c_cov=torch.cov(xc.T)\n",
    "print(f\"condition number: {torch.linalg.cond(x_c_cov)}\")\n",
    "print(\"\\n \\n correlation matrix and average cross correlation:\") \n",
    "cov_to_corr(x_c_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'eigen values of x.cov: {torch.linalg.eig(torch.cov(x.T))[0]}')\n",
    "print(f'eigen values of xc.cov: {torch.linalg.eig(torch.cov(xc.T))[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=torch.corrcoef(xc.T).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch whitening (Cholesky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cov matrix of xc.T\n",
      "tensor([[ 3.1477e+03, -3.1403e+02, -2.5581e+02,  ...,  4.4637e+02,\n",
      "         -3.5269e+01,  1.6182e+02],\n",
      "        [-3.1403e+02,  3.1783e+03, -7.7322e+01,  ...,  7.3933e+01,\n",
      "         -1.5754e+00,  3.6731e+02],\n",
      "        [-2.5581e+02, -7.7322e+01,  2.8857e+03,  ..., -2.0473e+02,\n",
      "         -6.7065e+01,  2.9217e+02],\n",
      "        ...,\n",
      "        [ 4.4637e+02,  7.3933e+01, -2.0473e+02,  ...,  3.0763e+03,\n",
      "         -2.1518e+02, -1.3461e+02],\n",
      "        [-3.5269e+01, -1.5754e+00, -6.7065e+01,  ..., -2.1518e+02,\n",
      "          4.0975e+03, -1.3811e+02],\n",
      "        [ 1.6182e+02,  3.6731e+02,  2.9217e+02,  ..., -1.3461e+02,\n",
      "         -1.3811e+02,  3.5700e+03]])\n",
      "cov matrix of x_w.T\n",
      "tensor([[ 1.0010e+00, -3.8185e-09,  1.2410e-08,  ...,  1.5583e-01,\n",
      "          2.7302e-03,  7.1518e-02],\n",
      "        [-3.8185e-09,  1.0010e+00,  3.3412e-09,  ...,  4.7767e-02,\n",
      "          6.6226e-04,  1.1858e-01],\n",
      "        [ 1.2410e-08,  3.3412e-09,  1.0010e+00,  ..., -3.8948e-02,\n",
      "         -3.4189e-02,  8.9340e-02],\n",
      "        ...,\n",
      "        [ 1.5583e-01,  4.7767e-02, -3.8948e-02,  ...,  1.0010e+00,\n",
      "         -7.6370e-09, -2.1002e-08],\n",
      "        [ 2.7302e-03,  6.6226e-04, -3.4189e-02,  ..., -7.6370e-09,\n",
      "          1.0010e+00, -2.6730e-08],\n",
      "        [ 7.1518e-02,  1.1858e-01,  8.9340e-02,  ..., -2.1002e-08,\n",
      "         -2.6730e-08,  1.0010e+00]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The outputs are not close enough!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcov(x_w\u001b[38;5;241m.\u001b[39mT))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Check if the outputs are indeed orthonormal\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(x_w\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mcov(), torch\u001b[38;5;241m.\u001b[39meye(num_features), atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe outputs are not close enough!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunctional validation passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: The outputs are not close enough!"
     ]
    }
   ],
   "source": [
    "# Our custom batch normalization layer\n",
    "bw_layer = BatchWhitening(num_features,num_groups=8,momentum=1)\n",
    "\n",
    "print('cov matrix of xc.T')\n",
    "print(torch.cov(xc.T))\n",
    "\n",
    "# Forward pass - expecting xc.shape=[N,D]\n",
    "x_w = bw_layer(xc)\n",
    "\n",
    "print('cov matrix of x_w.T')\n",
    "print(torch.cov(x_w.T))\n",
    "\n",
    "# Check if the outputs are indeed orthonormal\n",
    "assert torch.allclose(x_w.T.cov(), torch.eye(num_features), atol=1e-2), \"The outputs are not close enough!\"\n",
    "\n",
    "print(\"Functional validation passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_w_cov=torch.cov(x_w.T)\n",
    "x_w_cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True, True, True, True]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=2\n",
    "# x_w_cov[8*k:8*(k+1),8*k:8*(k+1)]\n",
    "[torch.allclose(x_w_cov[8*k:8*(k+1),8*k:8*(k+1)], torch.eye(8), atol=1e-2) for k in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the correlation matrix of the whitened signal and the average cross correlation\n",
    "cov_to_corr(x_w.T.cov())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the same correlation matrix as previous cell\n",
    "torch.corrcoef(x_w.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iternorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the same signal as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same signal \n",
    "print(f' xc.shape={xc.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ItN = IterNorm(num_features, num_groups=1, T=10,  momentum=1, affine=False)\n",
    "print(ItN)\n",
    "ItN.train()\n",
    "xc.requires_grad_()\n",
    "y = ItN(xc)  # y.shape=xc.shape=[N,D]\n",
    "# the following is an alternative computation of y's cov matrix (implements what cov is doing behind the scene with correction=0)\n",
    "z = y.transpose(0, 1).contiguous().view(x.size(1), -1) # z.shape=[D,N]\n",
    "y_cov = z.matmul(z.t()) / z.size(1)\n",
    "print(\"y_cov:\")\n",
    "print(y_cov)  # the outcome is the cov matrix at shape [D,D]\n",
    "\n",
    "cov_to_corr(y_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative computation \n",
    "x_w = ItN(xc)\n",
    "print(x_w.shape)\n",
    "x_w_cov = x_w.T.cov(correction=0)\n",
    "print(x_w_cov)\n",
    "\n",
    "# torch.corrcoef(x_w.T)\n",
    "cov_to_corr(x_w_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xc.T.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xc_corr=cov_to_corr(xc.T.cov())[0]\n",
    "xc_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2c=corr_to_cov(xc_corr,xc.std(axis=0))\n",
    "c2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.std(xc,dim=0)\n",
    "xc.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.randn(10, 3, 5)\n",
    "A = torch.zeros_like(M)\n",
    "batch1 = torch.randn(10, 3, 4)\n",
    "batch2 = torch.randn(10, 4, 5)\n",
    "B=torch.baddbmm(M, batch1, batch2,out=A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a(c):\n",
    "    a=0.9+0.1*torch.exp(-(c/0.9)**10)\n",
    "    torch.diagonal(a).fill_(1.0)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C=torch.corrcoef(xc.T).detach().numpy()\n",
    "C=torch.corrcoef(xc.T)\n",
    "print(C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
