{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet training on ImageNet\n",
    "the goal of this notebook is to train the EfficientNet model on the ImageNet dataset. \n",
    "\n",
    "- EfficientNet Model implementation: \n",
    "    - [Machine-Learning-Collection](https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master)\n",
    "    - [EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch/tree/master)\n",
    "\n",
    "- Imagenet dataset - from [Kaggle](https://www.kaggle.com/c/imagenet-object-localization-challenge/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device to use: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import models, datasets\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import os\n",
    "from random import randint\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "# Define device to use (CPU or GPU). CUDA = GPU support for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(\"Device to use:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/datasets/vision/imagenet/ILSVRC/Data/CLS-LOC' # Original images come in shapes of \n",
    "# Define training and validation data paths\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train') \n",
    "VALID_DIR = os.path.join(DATA_DIR, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup helper functions\n",
    "Taken from [this notebook](https://github.com/kennethleungty/PyTorch-Ignite-Tiny-ImageNet-Classification/blob/main/Tiny_ImageNet_Classification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to display single or a batch of sample images\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def show_batch(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = next(dataiter)\n",
    "    imshow(make_grid(images)) # Using Torchvision.utils make_grid function\n",
    "    \n",
    "def show_image(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = dataiter.next()\n",
    "    random_num = randint(0, len(images)-1)\n",
    "    imshow(images[random_num])\n",
    "    label = labels[random_num]\n",
    "    print(f'Label: {label}, Shape: {images[random_num].shape}')\n",
    "\n",
    "# Setup function to create dataloaders for image datasets\n",
    "def generate_dataloader(data, name, transform, batch_size=32):\n",
    "    if data is None: \n",
    "        return None\n",
    "    \n",
    "    # Read image files to pytorch dataset using ImageFolder, a generic data \n",
    "    # loader where images are in format root/label/filename\n",
    "    # See https://pytorch.org/vision/stable/datasets.html\n",
    "    if transform is None:\n",
    "        dataset = datasets.ImageFolder(data, transform=T.ToTensor())\n",
    "    else:\n",
    "        dataset = datasets.ImageFolder(data, transform=transform)\n",
    "\n",
    "    # Set options for device\n",
    "    if use_cuda:\n",
    "        kwargs = {\"pin_memory\": True, \"num_workers\": 1}\n",
    "    else:\n",
    "        kwargs = {}\n",
    "    \n",
    "    # Wrap image dataset (defined above) in dataloader \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
    "                        shuffle=(name==\"train\"), \n",
    "                        **kwargs)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize validation folder - do only once\n",
    "should be run if the validation folder is not present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the validation images folder\n",
    "validation_folder = VALID_DIR\n",
    "# Path to the folder containing XML annotations\n",
    "annotations_folder = \"/datasets/vision/imagenet/ILSVRC/Annotations/CLS-LOC/val\"\n",
    "\n",
    "# Create a dictionary to store image filenames and their corresponding classes\n",
    "image_classes = {}\n",
    "# Parse XML files to extract class labels for each image\n",
    "for xml_file in os.listdir(annotations_folder):\n",
    "    if xml_file.endswith(\".xml\"):\n",
    "        tree = ET.parse(os.path.join(annotations_folder, xml_file))\n",
    "        root = tree.getroot()\n",
    "        # Assuming there's only one object annotation per image\n",
    "        for obj in root.findall(\"object\"):\n",
    "            # Extract class label\n",
    "            class_label = obj.find(\"name\").text\n",
    "            # Extract image filename\n",
    "            image_filename = os.path.splitext(xml_file)[0] + \".JPEG\"\n",
    "            # Store image filename and class label\n",
    "            image_classes[image_filename] = class_label\n",
    "\n",
    "# Create subfolders for each class in the validation set\n",
    "for class_label in set(image_classes.values()):\n",
    "    class_folder = os.path.join(validation_folder, class_label)\n",
    "    os.makedirs(class_folder, exist_ok=True)\n",
    "\n",
    "# Move images to their corresponding class subfolders\n",
    "for image_filename, class_label in image_classes.items():\n",
    "    source_path = os.path.join(validation_folder, image_filename)\n",
    "    destination_path = os.path.join(validation_folder, class_label, image_filename)\n",
    "    shutil.move(source_path, destination_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save class names (for corresponding labels) as dict from words.txt file\n",
    "class_to_name_dict = dict()\n",
    "fp = '/datasets/vision/imagenet/LOC_synset_mapping.txt'\n",
    "data = fp.readlines()\n",
    "# for line in data:\n",
    "#     # words = line.strip('\\n').split('\\t')\n",
    "#     words = line.strip('\\n').split('\\t')\n",
    "#     class_to_name_dict[words[0]] = words[1].split(',')[0]\n",
    "# fp.close()\n",
    "\n",
    "# Display first 20 entries of resulting dictionary\n",
    "{k: class_to_name_dict[k] for k in list(class_to_name_dict)[:20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate training data statistics - do only once\n",
    "to be used for image normalization if training from scratch.  \n",
    "The result should be:\n",
    "```\n",
    "mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect statistics for the training data\n",
    "def calculate_mean_std(loader):\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    total_images_count = 0\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images_count += batch_samples\n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=T.Compose([T.ToTensor()]))\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = calculate_mean_std(train_loader)\n",
    "print(f'Mean: {mean}, Std: {std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Image Pre-processing Transformations\n",
    "PyTorch transforms define image transformations that convert all images in dataset into a standardized format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation sequence for image pre-processing\n",
    "# If not using pre-trained model, normalize with 0.5, 0.5, 0.5 (mean and SD)\n",
    "# If using pre-trained ImageNet, normalize with mean=[0.485, 0.456, 0.406], \n",
    "# std=[0.229, 0.224, 0.225])\n",
    "\n",
    "preprocess_transform = T.Compose([\n",
    "                T.Resize(256), # Resize images to 256 x 256\n",
    "                T.CenterCrop(224), # Center crop image\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),  # Converting cropped images to tensors\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a way to normalize after a transformetion that augment the data and generate multiple images (e.g. T.TenCrop)\n",
    "trn_mean=[0.485, 0.456, 0.406]\n",
    "trn_std=[0.229, 0.224, 0.225]\n",
    "normalize = T.Compose([T.ToTensor(), T.Normalize(mean=trn_mean, std=trn_std)])\n",
    "normalize_lambda = T.Lambda(lambda x: normalize(x[0]))\n",
    "\n",
    "preprocess_transform_tencrop = T.Compose([\n",
    "                T.Resize(256), # Resize images to 256 x 256\n",
    "                # T.CenterCrop(224), # Center crop image\n",
    "                # T.RandomHorizontalFlip(),\n",
    "                T.TenCrop(224),\n",
    "                normalize_lambda\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size for data loaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = generate_dataloader(TRAIN_DIR, \"train\",\n",
    "                                  transform=preprocess_transform,\n",
    "                                  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display batch of training set images\n",
    "# show_batch(train_loader)\n",
    "imgs,lbls = next(iter(train_loader))\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(make_grid(imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation set dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for validation data (depending if model is pretrained)\n",
    "val_loader = generate_dataloader(VALID_DIR, \"val\",\n",
    "                                 transform=preprocess_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training the model\n",
    "we'll be using the [Efficient-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch/tree/master) implementation of the EfficientNet model. and [Pytorch-lightning](https://lightning.ai/docs/pytorch/stable/) for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model\n",
    "I've copied the code from the Efficient-PyTorch implementation and saved it in the `efficientnet_pytorch` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "CHECKPOINT_PATH = \"saved_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet.from_name('efficientnet-b3', num_classes=200)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Lightning Module definition\n",
    "We'll be using the pytorch lightning library to train the model.  \n",
    "There are 2 references to follow:\n",
    "- [Pytorch Lightning tutorial 4](https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/04-inception-resnet-densenet.html)\n",
    "- [Image Classification using PTL and W&B](https://wandb.ai/wandb/wandb-lightning/reports/Image-Classification-Using-PyTorch-Lightning-and-Weights-Biases--VmlldzoyODk1NzY)\n",
    "\n",
    "following their [tutorial 4](https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/04-inception-resnet-densenet.html)\n",
    "we need to define 2 main modules:\n",
    "- TinyImageNetModule (LightningModule)\n",
    "- Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, WandbLogger\n",
    "L.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "\n",
    "def create_model(model_name, model_hparams):\n",
    "    if model_name in model_dict:\n",
    "        return model_dict[model_name](**model_hparams)\n",
    "    else:\n",
    "        assert False, f'Unknown model name \"{model_name}\". Available models are: {str(model_dict.keys())}'\n",
    "\n",
    "def create_efficientnet_model(model_name, model_hparams):\n",
    "    load_pretrained = model_hparams.pop('load_pretrained', False)\n",
    "    if load_pretrained:\n",
    "        model = EfficientNet.from_pretrained(model_name, **model_hparams)\n",
    "    else:\n",
    "        model = EfficientNet.from_name(model_name, **model_hparams)\n",
    "    return model\n",
    "\n",
    "model_hparams = {'load_pretrained': False, 'num_classes': 200}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImageNetModule(L.LightningModule):\n",
    "    def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams):\n",
    "        \"\"\"TinyImageNetModule.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model/CNN to run. Used for creating the model (see function below)\n",
    "            model_hparams: Hyperparameters for the model, as dictionary.\n",
    "            optimizer_name: Name of the optimizer to use. Currently supported: Adam, SGD\n",
    "            optimizer_hparams: Hyperparameters for the optimizer, as dictionary. This includes learning rate, weight decay, etc.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n",
    "        self.save_hyperparameters()\n",
    "        # Create model\n",
    "        self.model = create_efficientnet_model(model_name, model_hparams)\n",
    "        # Create loss module\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "        # Example input for visualizing the graph in Tensorboard\n",
    "        self.example_input_array = torch.zeros((1, 3, 224, 224), dtype=torch.float32)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        # Forward function that is run when visualizing the graph\n",
    "        return self.model(imgs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We will support Adam or SGD as optimizers.\n",
    "        if self.hparams.optimizer_name == \"Adam\":\n",
    "            # AdamW is Adam with a correct implementation of weight decay (see here\n",
    "            # for details: https://arxiv.org/pdf/1711.05101.pdf)\n",
    "            optimizer = optim.AdamW(self.parameters(), **self.hparams.optimizer_hparams)\n",
    "        elif self.hparams.optimizer_name == \"SGD\":\n",
    "            optimizer = optim.SGD(self.parameters(), **self.hparams.optimizer_hparams)\n",
    "        elif self.hparams.optimizer_name == \"RMSprop\":\n",
    "            optimizer = optim.RMSprop(self.parameters(), **self.hparams.optimizer_hparams)\n",
    "        else:\n",
    "            assert False, f'Unknown optimizer: \"{self.hparams.optimizer_name}\"'\n",
    "\n",
    "        # We will reduce the learning rate by 0.1 after 100 and 150 epochs\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # \"batch\" is the output of the training data loader.\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        # Logs the accuracy per epoch to tensorboard (weighted average over batches)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss  # Return tensor to call \".backward\" on\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs).argmax(dim=-1)\n",
    "        acc = (labels == preds).float().mean()\n",
    "        # By default logs it per epoch (weighted average over batches)\n",
    "        self.log(\"val_acc\", acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs).argmax(dim=-1)\n",
    "        acc = (labels == preds).float().mean()\n",
    "        # By default logs it per epoch (weighted average over batches), and returns it afterwards\n",
    "        self.log(\"test_acc\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, trainer_params, save_name=None, eval_only=False,**kwargs):\n",
    "    \"\"\"Train model.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
    "        save_name (optional): If specified, this name will be used for creating the checkpoint and logging directory.\n",
    "    \"\"\"\n",
    "    save_name = model_name+save_name\n",
    "\n",
    "    # add wandb logger\n",
    "    wandb_logger = WandbLogger(name=save_name, project=\"tiny-imagenet\")\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),  # Where to save models and tensorboard logs\n",
    "        # We run on a single GPU (if possible)\n",
    "        accelerator=\"auto\",\n",
    "        # devices=trainer_params['devices'],\n",
    "        # strategy=trainer_params['strategy'],\n",
    "        # # How many epochs to train for if no patience is set\n",
    "        # max_epochs=trainer_params[\"max_epochs\"],\n",
    "        logger = wandb_logger,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(\n",
    "                save_weights_only=True, mode=\"max\", monitor=\"val_acc\"\n",
    "            ),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "        ],  # Log learning rate every epoch\n",
    "        **trainer_params\n",
    "    )  # In case your notebook crashes due to the progress bar, consider increasing the refresh rate\n",
    "    trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, save_name + \".ckpt\")\n",
    "    if eval_only and os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        # Automatically loads the model with the saved hyperparameters\n",
    "        model = TinyImageNetModule.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        L.seed_everything(42)  # To be reproducible\n",
    "        model = TinyImageNetModule(model_name=model_name, **kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = TinyImageNetModule.load_from_checkpoint(\n",
    "            trainer.checkpoint_callback.best_model_path\n",
    "        )  # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "    # test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    # result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    result = {\"val\": val_result[0][\"test_acc\"]}\n",
    "    wandb.finish()\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hparams = {'load_pretrained': False, 'num_classes': 200}\n",
    "optimizer_hparams = {'lr': 0.01, 'weight_decay': 0.0001}\n",
    "optimizer_name = \"RMSprop\"\n",
    "# trainer_params = {'max_epochs': 10,'devices':'auto','strategy':'ddp'}\n",
    "trainer_params = {'max_epochs': 50,'devices':'auto'}\n",
    "\n",
    "train_model('efficientnet-b0', trainer_params, save_name='exp2',model_hparams=model_hparams, optimizer_name=optimizer_name, optimizer_hparams=optimizer_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./saved_models/efficientnet-b0exp1/lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[\"EfficientNetb0\"] = GoogleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20019"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many iterations in a batch\n",
    "iters = len(train_loader)\n",
    "iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1281167"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=50\n",
    "n_cycles = 5\n",
    "\n",
    "T_0= (n_epochs*iters)//n_cycles     # T_0 is the number of iterations in the first cycle.\n",
    "T_mult=1\n",
    "model_hparams = {'num_classes': 200}\n",
    "optimizer_hparams = {'lr': 0.01, 'weight_decay': 0.0001}\n",
    "\n",
    "model = EfficientNet.from_name('efficientnet-b0', **model_hparams)\n",
    "optimizer = optim.AdamW(model.parameters(), **optimizer_hparams)\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult,eta_min=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs=[]\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(iters):\n",
    "        scheduler.step()\n",
    "        lrs.append(scheduler.get_last_lr()[0])\n",
    "\n",
    "# plot the learning rate schedule using plotly\n",
    "fig = go.Figure()\n",
    "# fig.add_trace(go.Scatter(x=list(range(len(lrs))), y=lrs, mode='lines+markers'))\n",
    "fig.add_trace(go.Scatter(x=list(range(len(lrs))), y=lrs, mode='lines'))\n",
    "fig.update_layout(title='Learning Rate Schedule',\n",
    "                   xaxis_title='Iteration',\n",
    "                   yaxis_title='Learning Rate')\n",
    "fig.show()\n",
    "\n",
    "# plt.plot(lrs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
