{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fda2113",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T12:55:55.990900Z",
     "start_time": "2021-12-27T12:55:55.871351Z"
    }
   },
   "source": [
    "# Image Classification with Vision Transformers\n",
    "in this notebook we'll go through some code examples of various transformers.  \n",
    "we'll either implement the architecture from scratch using pytorch (following [keras code examples](https://keras.io/examples/)) or use the implementation from [vit-pytorch](https://github.com/guyk1971/vit-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3394e5c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T16:56:52.276240Z",
     "start_time": "2021-12-28T16:56:52.260547Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fbf05e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T16:56:53.485666Z",
     "start_time": "2021-12-28T16:56:52.851098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.core.display import display, HTML\n",
    "# from IPython.core.debugger import set_trace\n",
    "display(HTML('<style>.container { width:75% !important; }</style>')) \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f28a23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T16:56:55.269332Z",
     "start_time": "2021-12-28T16:56:54.806271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as T\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c37faf",
   "metadata": {},
   "source": [
    "# Image Classification with ViT\n",
    "Reference: [keras example](https://keras.io/examples/vision/image_classification_with_vision_transformer/)  \n",
    "In this section we'll implement the [Vision transformer](https://arxiv.org/abs/2010.11929) on CIFAR100 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b541c70",
   "metadata": {},
   "source": [
    "## prepare the data\n",
    "including the transforms that has to be done when accessing items in the dataset (part of the `_get_item()` method)  \n",
    "see [cifar10 tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) for a demo how compose transforms. we'll do the same normalization here.  \n",
    "\n",
    "another reference : [pytorch cifar](https://github.com/kuangliu/pytorch-cifar)\n",
    "\n",
    "for the normalization, one has to calculate the mean and stdev of the dataset. this can be done according to this [blog post](https://towardsdatascience.com/how-to-calculate-the-mean-and-standard-deviation-normalizing-datasets-in-pytorch-704bd7d05f4c)  \n",
    "Other than the normalization, the keras example has some additional transformations. to follow the flow of the keras example, we'll start with defining the CIFAR100 class without these data augmentation transforms. we'll add them in a following subsection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f5c3f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T16:57:00.995772Z",
     "start_time": "2021-12-28T16:56:59.760560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "x_train shape: (50000, 32, 32, 3) - y_train shape: 50000\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: 10000\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "training_data = datasets.CIFAR100(\n",
    "    root=\"~/hda/data/cifar-100\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=T.ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR100(\n",
    "    root=\"~/hda/data/cifar-100\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=T.ToTensor()\n",
    ")\n",
    "\n",
    "print(f\"x_train shape: {training_data.data.shape} - y_train shape: {len(training_data.targets)}\")\n",
    "print(f\"x_test shape: {test_data.data.shape} - y_test shape: {len(test_data.targets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c16b903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T16:57:10.294192Z",
     "start_time": "2021-12-28T16:57:05.389883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.50704783, 0.48648766, 0.4408386) (0.26733738, 0.25643668, 0.27614135)\n"
     ]
    }
   ],
   "source": [
    "# Calculating the statistics for normalization: https://towardsdatascience.com/how-to-calculate-the-mean-and-standard-deviation-normalizing-datasets-in-pytorch-704bd7d05f4c\n",
    "train_dataloader = DataLoader(dataset=training_data, batch_size=64)\n",
    "\n",
    "def get_mean_and_std(dataloader):\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    for data, _ in dataloader:\n",
    "        # Mean over batch, height and width, but not over the channels\n",
    "        channels_sum += torch.mean(data, dim=[0,2,3])\n",
    "        channels_squared_sum += torch.mean(data**2, dim=[0,2,3])\n",
    "        num_batches += 1\n",
    "    \n",
    "    mean = channels_sum / num_batches\n",
    "\n",
    "    # std = sqrt(E[X^2] - (E[X])^2)\n",
    "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "mean,std = get_mean_and_std(train_dataloader)\n",
    "train_mean=tuple(mean.cpu().numpy())\n",
    "train_std=tuple(std.cpu().numpy())\n",
    "print(train_mean,train_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa160d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T13:43:05.755639Z",
     "start_time": "2021-12-27T13:43:05.648758Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the data\n",
    "img,lbl=training_data[0]\n",
    "plt.imshow(img.permute(1,2,0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bcfd70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T14:07:59.469773Z",
     "start_time": "2021-12-27T14:07:59.429423Z"
    }
   },
   "outputs": [],
   "source": [
    "img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60bfca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T19:19:07.693382Z",
     "start_time": "2021-12-27T19:19:07.661517Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf7e33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T15:14:01.888554Z",
     "start_time": "2021-12-27T15:14:01.842135Z"
    }
   },
   "outputs": [],
   "source": [
    "img.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb800972",
   "metadata": {},
   "source": [
    "## Configure the Hyperparameters\n",
    "configure as follows: \n",
    "```\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87863764",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T16:57:18.656486Z",
     "start_time": "2021-12-28T16:57:18.624401Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01643b2",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "```\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n",
    "```\n",
    "\n",
    "Lets write the data_augmentation module in pytorch. \n",
    "following the example from \"Modern Computer Vision with Pytorch\" chapter 06, we'll define the data augmentation for pytorch - they have used a special package there but recommended the `transforms` module in pytorch. so let's use that. [transforms documentation](https://pytorch.org/vision/stable/transforms.html#transforms-on-pil-image-and-torch-tensor)  \n",
    "Note : for an illustration of pytorch transformations see [this page](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Normalization** - not sure that we need the normalization. the CIFAR100 dataset already normalizes the input as part of the `__get_item__()` method. according to the [cifar10 tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) : \"_The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1]._\". the normalization is done by first transforming them [`ToTensor`](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor) (which transform them to float at [0,1]) followed by the normalization with (0.5,0.5,0.5) for mean and stdev\n",
    "\n",
    "\n",
    "see also this [data loadting tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb823ff5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T16:57:26.482638Z",
     "start_time": "2021-12-28T16:57:25.241129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "x_train shape: (50000, 32, 32, 3) - y_train shape: 50000\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: 10000\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(train_mean, train_std),\n",
    "        T.Resize((image_size,image_size)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomRotation(5),\n",
    "        ])\n",
    "\n",
    "# lets redefine the datasets, now with the data_augmentation transforms\n",
    "training_data = datasets.CIFAR100(\n",
    "    root=\"~/hda/data/cifar-100\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=data_augmentation\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR100(\n",
    "    root=\"~/hda/data/cifar-100\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=data_augmentation\n",
    ")\n",
    "\n",
    "print(f\"x_train shape: {training_data.data.shape} - y_train shape: {len(training_data.targets)}\")\n",
    "print(f\"x_test shape: {test_data.data.shape} - y_test shape: {len(test_data.targets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22af4b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T09:52:14.986905Z",
     "start_time": "2021-12-28T09:52:14.884058Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the data\n",
    "img,lbl=training_data[2]\n",
    "plt.imshow(img.permute(1,2,0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d4a65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T09:52:33.281180Z",
     "start_time": "2021-12-28T09:52:33.148000Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(training_data.data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e95457ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T16:57:29.071922Z",
     "start_time": "2021-12-28T16:57:29.059821Z"
    }
   },
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "train_dataloader = DataLoader(dataset=training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b7597c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T19:15:16.047661Z",
     "start_time": "2021-12-27T19:15:16.002037Z"
    }
   },
   "source": [
    "## Creating a ViT Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27913d1e",
   "metadata": {},
   "source": [
    "### Using `vit-pytorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cbcdd5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T16:57:33.091552Z",
     "start_time": "2021-12-28T16:57:33.046540Z"
    }
   },
   "outputs": [],
   "source": [
    "from vit_pytorch import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5323ea4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T16:57:34.294763Z",
     "start_time": "2021-12-28T16:57:34.284683Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    vit = ViT(image_size = image_size,\n",
    "              patch_size = patch_size,\n",
    "              num_classes = num_classes,\n",
    "              dim = projection_dim,\n",
    "              depth = transformer_layers,\n",
    "              heads = num_heads,\n",
    "              mlp_dim = 2048,\n",
    "              dropout = 0.1,\n",
    "              emb_dropout = 0.1)\n",
    "    return vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df0ceaff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T16:57:35.847755Z",
     "start_time": "2021-12-28T16:57:35.835608Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "def test_epoch(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33f3acb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-28T17:05:51.380125Z",
     "start_time": "2021-12-28T16:57:38.352343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/50000 (0%)]\t training loss: 0.053128\n",
      "epoch: 1 [2560/50000 (5%)]\t training loss: -0.899818\n",
      "epoch: 1 [5120/50000 (10%)]\t training loss: -1.341441\n",
      "epoch: 1 [7680/50000 (15%)]\t training loss: -1.907683\n",
      "epoch: 1 [10240/50000 (20%)]\t training loss: -2.534189\n",
      "epoch: 1 [12800/50000 (26%)]\t training loss: -3.171210\n",
      "epoch: 1 [15360/50000 (31%)]\t training loss: -4.052217\n",
      "epoch: 1 [17920/50000 (36%)]\t training loss: -4.713503\n",
      "epoch: 1 [20480/50000 (41%)]\t training loss: -5.565493\n",
      "epoch: 1 [23040/50000 (46%)]\t training loss: -6.409640\n",
      "epoch: 1 [25600/50000 (51%)]\t training loss: -7.430694\n",
      "epoch: 1 [28160/50000 (56%)]\t training loss: -8.420911\n",
      "epoch: 1 [30720/50000 (61%)]\t training loss: -9.451516\n",
      "epoch: 1 [33280/50000 (66%)]\t training loss: -10.553101\n",
      "epoch: 1 [35840/50000 (71%)]\t training loss: -11.685748\n",
      "epoch: 1 [38400/50000 (77%)]\t training loss: -12.694371\n",
      "epoch: 1 [40960/50000 (82%)]\t training loss: -14.108666\n",
      "epoch: 1 [43520/50000 (87%)]\t training loss: -15.282427\n",
      "epoch: 1 [46080/50000 (92%)]\t training loss: -16.586393\n",
      "epoch: 1 [48640/50000 (97%)]\t training loss: -18.046274\n",
      "\n",
      "Test dataset: Overall Loss: -18.9049, Overall Accuracy: 100/10000 (1%)\n",
      "\n",
      "epoch: 2 [0/50000 (0%)]\t training loss: -18.823605\n",
      "epoch: 2 [2560/50000 (5%)]\t training loss: -20.293171\n",
      "epoch: 2 [5120/50000 (10%)]\t training loss: -21.682487\n",
      "epoch: 2 [7680/50000 (15%)]\t training loss: -23.276756\n",
      "epoch: 2 [10240/50000 (20%)]\t training loss: -24.875544\n",
      "epoch: 2 [12800/50000 (26%)]\t training loss: -26.523506\n",
      "epoch: 2 [15360/50000 (31%)]\t training loss: -28.388226\n",
      "epoch: 2 [17920/50000 (36%)]\t training loss: -30.021828\n",
      "epoch: 2 [20480/50000 (41%)]\t training loss: -31.791628\n",
      "epoch: 2 [23040/50000 (46%)]\t training loss: -33.640419\n",
      "epoch: 2 [25600/50000 (51%)]\t training loss: -35.586609\n",
      "epoch: 2 [28160/50000 (56%)]\t training loss: -37.590469\n",
      "epoch: 2 [30720/50000 (61%)]\t training loss: -39.626797\n",
      "epoch: 2 [33280/50000 (66%)]\t training loss: -41.708168\n",
      "epoch: 2 [35840/50000 (71%)]\t training loss: -43.806915\n",
      "epoch: 2 [38400/50000 (77%)]\t training loss: -45.696815\n",
      "epoch: 2 [40960/50000 (82%)]\t training loss: -48.184952\n",
      "epoch: 2 [43520/50000 (87%)]\t training loss: -50.314640\n",
      "epoch: 2 [46080/50000 (92%)]\t training loss: -52.616314\n",
      "epoch: 2 [48640/50000 (97%)]\t training loss: -55.152981\n",
      "\n",
      "Test dataset: Overall Loss: -56.5573, Overall Accuracy: 100/10000 (1%)\n",
      "\n",
      "epoch: 3 [0/50000 (0%)]\t training loss: -56.485836\n",
      "epoch: 3 [2560/50000 (5%)]\t training loss: -59.004856\n",
      "epoch: 3 [5120/50000 (10%)]\t training loss: -61.308815\n",
      "epoch: 3 [7680/50000 (15%)]\t training loss: -63.997746\n",
      "epoch: 3 [10240/50000 (20%)]\t training loss: -66.588188\n",
      "epoch: 3 [12800/50000 (26%)]\t training loss: -69.238869\n",
      "epoch: 3 [15360/50000 (31%)]\t training loss: -72.216614\n",
      "epoch: 3 [17920/50000 (36%)]\t training loss: -74.770615\n",
      "epoch: 3 [20480/50000 (41%)]\t training loss: -77.571335\n",
      "epoch: 3 [23040/50000 (46%)]\t training loss: -80.453186\n",
      "epoch: 3 [25600/50000 (51%)]\t training loss: -83.392082\n",
      "epoch: 3 [28160/50000 (56%)]\t training loss: -86.422272\n",
      "epoch: 3 [30720/50000 (61%)]\t training loss: -89.562958\n",
      "epoch: 3 [33280/50000 (66%)]\t training loss: -92.663277\n",
      "epoch: 3 [35840/50000 (71%)]\t training loss: -95.684601\n",
      "epoch: 3 [38400/50000 (77%)]\t training loss: -98.479645\n",
      "epoch: 3 [40960/50000 (82%)]\t training loss: -102.048233\n",
      "epoch: 3 [43520/50000 (87%)]\t training loss: -105.167488\n",
      "epoch: 3 [46080/50000 (92%)]\t training loss: -108.472511\n",
      "epoch: 3 [48640/50000 (97%)]\t training loss: -112.129242\n",
      "\n",
      "Test dataset: Overall Loss: -114.0480, Overall Accuracy: 100/10000 (1%)\n",
      "\n",
      "epoch: 4 [0/50000 (0%)]\t training loss: -114.045616\n",
      "epoch: 4 [2560/50000 (5%)]\t training loss: -117.602982\n",
      "epoch: 4 [5120/50000 (10%)]\t training loss: -120.716507\n",
      "epoch: 4 [7680/50000 (15%)]\t training loss: -124.538696\n",
      "epoch: 4 [10240/50000 (20%)]\t training loss: -128.026337\n",
      "epoch: 4 [12800/50000 (26%)]\t training loss: -131.686127\n",
      "epoch: 4 [15360/50000 (31%)]\t training loss: -135.741302\n",
      "epoch: 4 [17920/50000 (36%)]\t training loss: -139.171051\n",
      "epoch: 4 [20480/50000 (41%)]\t training loss: -142.997772\n",
      "epoch: 4 [23040/50000 (46%)]\t training loss: -146.877121\n",
      "epoch: 4 [25600/50000 (51%)]\t training loss: -150.774979\n",
      "epoch: 4 [28160/50000 (56%)]\t training loss: -154.810913\n",
      "epoch: 4 [30720/50000 (61%)]\t training loss: -159.021637\n",
      "epoch: 4 [33280/50000 (66%)]\t training loss: -163.107086\n",
      "epoch: 4 [35840/50000 (71%)]\t training loss: -166.988007\n",
      "epoch: 4 [38400/50000 (77%)]\t training loss: -170.632309\n",
      "epoch: 4 [40960/50000 (82%)]\t training loss: -175.235748\n",
      "epoch: 4 [43520/50000 (87%)]\t training loss: -179.314926\n",
      "epoch: 4 [46080/50000 (92%)]\t training loss: -183.588211\n",
      "epoch: 4 [48640/50000 (97%)]\t training loss: -188.333481\n",
      "\n",
      "Test dataset: Overall Loss: -190.7753, Overall Accuracy: 100/10000 (1%)\n",
      "\n",
      "epoch: 5 [0/50000 (0%)]\t training loss: -190.832428\n",
      "epoch: 5 [2560/50000 (5%)]\t training loss: -195.399841\n",
      "epoch: 5 [5120/50000 (10%)]\t training loss: -199.230408\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0f3928c52152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mvit_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vit_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvit_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-0f3928c52152>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtest_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-02a79ec02c7b>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, device, train_dataloader, optim, epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# nll is the negative likelihood loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mb_i\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
      "\u001b[0;32m~/anaconda3/envs/ptvit/lib/python3.8/site-packages/torch-1.10.1-py3.8-linux-x86_64.egg/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ptvit/lib/python3.8/site-packages/torch-1.10.1-py3.8-linux-x86_64.egg/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ptvit/lib/python3.8/site-packages/torch-1.10.1-py3.8-linux-x86_64.egg/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             F.adamw(params_with_grad,\n\u001b[0m\u001b[1;32m    138\u001b[0m                     \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ptvit/lib/python3.8/site-packages/torch-1.10.1-py3.8-linux-x86_64.egg/torch/optim/_functional.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_experiment(model):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(1, num_epochs):\n",
    "        train_epoch(model, device, train_dataloader, optimizer, epoch)\n",
    "        test_epoch(model, device, test_dataloader)\n",
    "    \n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "\n",
    "history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50f55fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
